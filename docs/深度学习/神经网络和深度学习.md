
# 神经网络和深度学习

## 深度学习概论

深度学习指的是训练神经网络。  
ReLU函数，全称是修正线性单元（rectified linear unit），它是一条初始一段时间内为0，之后是一条直线的函数，如图。   
<div align="center"><img src="./pictures/Andrew/ReLU_function.png" width="40%"/></div>  
修正指的是在初始一段时间内取不小于零的值。  
supervised learnning ，即监督学习。  
nature language processing,自然语言文字处理NLP。  
在图像处理中，经常使用卷积神经网络CNN。  
对于序列数据，如音频，由于包含了时间组件，所以语音信号作为一维时间序列，经常使用递归神经网络RNN（Recurrent Neural Network），语言最自然的表示方式也是序列数据。  
结构化数据，数据的基本数据库，每个特征都有一个很好的定义，例如在房价预测中，你可能有一个数据库，有专门的几列数据告诉你卧室的大小和数量，这就是结构化数据。  
非结构化数据，比如音频，原始音频或者你想要识别的图像或文本中的内容，这里的特征可能是图像中的像素值或文本中的单个单词。  
要使神经网络获得较高的性能体现，那么有两个条件：一是训练一个规模足够大的神经网络，以发挥数据规模量巨大的优点，二是需要大量的数据。  
forward propagation : 前向传播  
backward propagation ：反向传播

## 神经网络基础

### 符号定义
𝑥：表示一个𝑛𝑥维数据，为输入数据，维度为(𝑛𝑥, 1)；  
𝑦：表示输出结果，在二分类情况下取值为(0,1)；  
(𝑥(𝑖), 𝑦(𝑖))：表示第𝑖组数据，可能是训练数据，也可能是测试数据，此处默认为训练数据；  
𝑋 = [𝑥(1), 𝑥(2), . . . , 𝑥(𝑚)]：表示所有的训练数据集的输入值，放在一个 𝑛𝑥 × 𝑚的矩阵中，其中𝑚表示样本数目;   
𝑌 = [𝑦(1), 𝑦(2), . . . , 𝑦(𝑚)]：对应表示所有训练数据集的输出值，维度为1 × 𝑚。  
<div align="center"><img src="./pictures/Andrew/symbol_definition_X.png" width="40%"/></div>  

### 二分分类
二分分类就是输入特征向量后，预测输出结果为0还是1。例如，在猫分类问题上，我们以图片的特征向量作为输入，然后预测输出结果𝑦，0表示没有猫，而1表示有猫。  

### 逻辑回归(Logistic Regression)

逻辑回归学习算法，该算法适用于二分类问题。  
对于二元分类问题，给定一个输入特征向量X，输出预测结果 $\hat{y}$，$\hat{y}$ 也就是对实际值y的估计。换句话来说，如果𝑋是我们在上个视频看到的图片，你想让 $\hat{y}$ 来告诉你这是一只猫的图片的机率有多大。  
𝑋是一个𝑛𝑥维的向量（相当于有𝑛𝑥个特征的特征向量），我们用𝑤来表示逻辑回归的参数，这也是一个𝑛𝑥维向量（因为𝑤实际上是特征权重，维度与特征向量相同），参数里面还有𝑏，这是一个实数（表示偏差）。  
在做线性回归时可以做关于输入的线性函数 $\hat{y}=𝑤𝑇𝑥+𝑏$,但是这对于二元分类问题来说不是一个非常好的算法，因为想让 $\hat{y}$ 表示实际值𝑦等于1的机率的话，$\hat{y}$ 应该在0到1之间，而 𝑤𝑇𝑥 + 𝑏 可能比1要大得多，或者甚至为一个负值。因此在逻辑回归中，我们的输出应该是 $\hat{y}$ 等于由上面得到的线性函数式子作为自变量的 sigmoid 函数中，公式如下右图所示，将线性函数转换为非线性函数。  
<img src="./pictures/Andrew/sigma_function.png" width="40%"/>
<img src="./pictures/Andrew/sigma_definition.png" width="55%"/>  
因此当我们实现逻辑回归时，我们的工作就是去让机器学习参数𝑤以及𝑏这样才使得 $\hat{y}$ 成为对 𝑦=1 这一情况的概率的一个很好的估计。  

### Logistic回归损失函数

损失函数又叫做误差函数，用来衡量算法的运行情况，衡量预测输出值和实际值有多接近，Loss function:$𝐿(\hat{y} , 𝑦)$。  
一般情况下损失函数我们用预测值和实际值的平方差或者它们平方差的一半来定义，但是通常在逻辑回归中我们不这么做，因为当我们在学习逻辑回归参数w和b的时候，会发现我们的优化目标不是凸优化，只能找到多个局部最优值，梯度下降法很可能找不到全局最优值。  
在逻辑回归中用的损失函数是：$𝐿(\hat{y} , 𝑦) = −𝑦log(\hat{y}) − (1 − 𝑦)log(1 − \hat{y})$  
为什么要用这个函数作为逻辑损失函数？当我们使用平方误差作为损失函数的时候，你会想要让这个误差尽可能地小，对于这个逻辑回归损失函数，我们也想让它尽可能地小。上述的损失函数当𝑦 = 1时损失函数$𝐿 = −log(\hat{y})$，如果想要损失函数𝐿尽可能得小，那么𝑦^就要尽可能大，因为 sigmoid 函数取值[0,1]，所以𝑦^会无限接近于 1。 当𝑦 = 0时损失函数$𝐿 = −log(1 − \hat{y})$，如果想要损失函数𝐿尽可能得小，那么$\hat{y}$就要尽可能小，因为 sigmoid 函数取值[0,1]，所以$\hat{y}$会无限接近于 0。  
有很多的函数效果和上述函数类似，就是如果𝑦等于 1，我们就尽可能让 $\hat{y}$ 变大，如果𝑦等于 0，我们就尽可能让 $\hat{y}$ 变小。  
损失函数是在单个训练样本中定义的，它衡量的是算法在单个训练样本中表现如何，为了衡量算法在全部训练样本上的表现如何，我们需要定义一个算法的代价函数，算法的代价函数是对𝑚个样本的损失函数求和然后除以𝑚:  
<div align="center"><img src="./pictures/Andrew/cost_function.png" width="90%"/></div>  
损失函数只适用于像这样的单个训练样本，而代价函数是参数的总代价，所以在训练逻辑回归模型时候，我们需要找到合适的𝑤和𝑏，来让代价函数 𝐽 的总代价降到最低。