<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>卷积神经网络 | Hello World</title>
    <meta name="generator" content="VuePress 1.8.2">
    
    <meta name="description" content="Just playing around">
    
    <link rel="preload" href="/assets/css/0.styles.181f12b1.css" as="style"><link rel="preload" href="/assets/js/app.2c38286c.js" as="script"><link rel="preload" href="/assets/js/2.a44dd1be.js" as="script"><link rel="preload" href="/assets/js/18.215bea53.js" as="script"><link rel="prefetch" href="/assets/js/10.47078729.js"><link rel="prefetch" href="/assets/js/11.636591c1.js"><link rel="prefetch" href="/assets/js/12.aea696f8.js"><link rel="prefetch" href="/assets/js/13.7106f9a1.js"><link rel="prefetch" href="/assets/js/14.5a5a3f55.js"><link rel="prefetch" href="/assets/js/15.069e1835.js"><link rel="prefetch" href="/assets/js/16.7c753b28.js"><link rel="prefetch" href="/assets/js/17.1a8eabd9.js"><link rel="prefetch" href="/assets/js/3.03f8c983.js"><link rel="prefetch" href="/assets/js/4.e7c996f5.js"><link rel="prefetch" href="/assets/js/5.6eff8140.js"><link rel="prefetch" href="/assets/js/6.17d5dd20.js"><link rel="prefetch" href="/assets/js/7.a310a13e.js"><link rel="prefetch" href="/assets/js/8.3a2c2aec.js"><link rel="prefetch" href="/assets/js/9.de61ec54.js">
    <link rel="stylesheet" href="/assets/css/0.styles.181f12b1.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Hello World</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <!----></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>matlab</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>python</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>无人机项目</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>深度学习</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/深度学习/卷积神经网络.html" class="active sidebar-link">卷积神经网络</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/深度学习/卷积神经网络.html#卷积神经网络" class="sidebar-link">卷积神经网络</a></li></ul></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>数字图像处理</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h2 id="卷积神经网络"><a href="#卷积神经网络" class="header-anchor">#</a> 卷积神经网络</h2> <p><strong>1、卷积神经网络通常包含以下几种层：</strong></p> <ul><li><strong>卷积层</strong>（Convolutional layer）：卷积神经网路中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过反向传播算法优化得到的。卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。</li> <li><strong>线性整流层</strong>（Rectified Linear Units layer, ReLU layer）：这一层神经的活性化函数（Activation function）使用线性整流（Rectified Linear Units, ReLU）。</li> <li><strong>池化层</strong>（Pooling layer）：通常在卷积层之后会得到维度很大的特征，将特征切成几个区域，取其最大值或平均值，得到新的、维度较小的特征。</li> <li><strong>全连接层</strong>（Fully-Connected layer）：把所有局部特征结合变成全局特征，用来计算最后每一类的得分。</li></ul> <p><strong>2、卷积层（Convolutional layer）</strong></p> <ul><li><strong>Features</strong>，卷积层中用来跟图像进行卷积操作的小块，可以看作一个小图。所谓卷积操作就是将图像与 Features 的位置一一对应相乘，再将其相加除以 Features 的个数，这个值作为当前图像与 Features 的卷积值，随着 Features 跟着图像滑动，这些卷积值也形成了一个二维数组，这个二维数组就是卷积操作的结果，也可以理解为对原始图像过滤的结果，我们称之为 feature map，它是每一个 feature 从原始图像中提取出来的“特征”。其中的值，越接近为1表示对应位置和 feature 的匹配越完整，越是接近-1，表示对应位置和 feature 的反面匹配越完整，而值接近0的表示对应位置没有任何匹配或者说没有什么关联。原始图经过不同 feature 的卷积操作就变成了一系列的feature map。这就是卷积层的操作，也是卷积神经网络的由来。</li> <li><strong>神经元</strong>，在一个 Feature 和整个图像的卷积过程中需要进行的卷积次数，Feature 每滑动一次进行一次卷积操作。</li> <li><strong>连接数</strong>，在单次卷积操作中，feature 和图像对应相乘的次数，再加上阈值（或偏移量）的个数（一般为1），如5*5的 feature，单次卷积中需要进行5*5=25次相乘，再加上阈值为1，总共26，所以连接数为26。</li></ul> <p><strong>3、线性整流层（ReLU layer）</strong><br>
ReLu层是利用一个激活函数（或叫做修正线性单元）进行修正的，其数学公式为：<code>f(x)=max(0,x)</code>，也就是说，对于输入的负值，输出全为0，对于正值，原样输出。</p> <p><strong>4、池化层（Pooling Layer）</strong><br>
池化就是将输入图像进行缩小，减少像素信息，只保留重要信息。通常情况下，池化都是2*2大小，比如对于 max-pooling 来说，就是取输入图像中2*2大小的块中的最大值，作为结果的像素值，相当于将原始图像缩小了4倍 (注：同理，对于 average-pooling 来说，就是取2*2大小块的平均值作为结果的像素值)。因为最大池化（max-pooling）保留了每一个小块内的最大值，所以它相当于保留了这一块最佳的匹配结果，即它能发现是否匹配，也不会在意是哪一个部分匹配。</p> <p><strong>5、全连接层（Fully-Connected Layer）</strong><br>
将卷积结果排成一列向量<code>x{i}</code>，分别与输出两个神经元（X、O）全连接，这样一来就有12*2=24个连接，每个连接都有一个权重<code>w{i}</code>,根据<code>y=sum(w_{i}*x_{i})</code>分别求出两个神经元的输出，一般而言输出结果通过一个逻辑函数，将结果缩放到0—1之间，然后对两个结果进行判定，取较大的值（如取X）。<br>
个人理解最后两个输出神经元对应的是可以进行判定的标准，比如我们进行完卷积神经网络识别后，要将其判断为猫或者狗，猫和狗都是我们进行判定的标准，感觉这个应该是事先训练好的。</p> <p><strong>6、卷积神经网络结构</strong><br>
一般可以有多种嵌套方式，如以下：</p> <div class="language- extra-class"><pre class="language-text"><code>原图 -&gt; 卷积层 -&gt; ReLu层 -&gt; 卷积层 -&gt; ReLu层 -&gt; 池化层 -&gt; 卷积层 -&gt; ReLu层 -&gt; 池化层 -&gt; 全连接层 -&gt; 全连接层 -&gt; 判定  
</code></pre></div><p>这一整个过程，从前往后，被称作“前向传播”，得到一组输出，然后通过反向传播来不断纠正错误，进行学习。</p> <p><strong>7、反向传播算法BP</strong><br>
所谓反向传播，就是对比预测值和真实值，继而返回去修改网络参数的过程，</p> <p><strong>8、数据预处理</strong></p> <ul><li><p>去均值<br>
各维度都减对应维度的均值，使得输入数据各个维度都中心化为0，进行去均值的原因是因为如果不去均值的话会容易拟合。这是因为如果在神经网络中，特征值<code>x</code>比较大的时候，会导致<code>W*x+b</code>的结果也会很大，这样进行激活函数（如relu）输出时，会导致对应位置数值变化量太小，进行反向传播时因为要使用这里的梯度进行计算，所以会导致梯度消散问题，导致参数改变量很小，也就会易于拟合，效果不好。</p></li> <li><p>归一化<br>
进行归一化的原因是把各个特征的尺度控制在相同的范围内，这样可以便于找到最优解，能提高收敛效率，省事多了。<br>
（1）最值归一化，比如把最大值归一化成1，最小值归一化成-1；或把最大值归一化成1，最小值归一化成0。适用于本来就分布在有限范围内的数据。<br>
（2）均值方差归一化，一般是把均值归一化成0，方差归一化成1。适用于分布没有明显边界的情况。</p></li></ul> <p><strong>9、名词解释</strong><br>
（1）鲁棒性：robust的音译，即健壮性，指系统在错误操作下，能够保持正确运行的能力。</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/无人机项目/ROS学习心得.html" class="prev">
        ROS学习心得
      </a></span> <span class="next"><a href="/数字图像处理/基础概述.html">
        基础概述
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.2c38286c.js" defer></script><script src="/assets/js/2.a44dd1be.js" defer></script><script src="/assets/js/18.215bea53.js" defer></script>
  </body>
</html>
